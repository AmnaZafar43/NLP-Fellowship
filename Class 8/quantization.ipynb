{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install optimum\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from optimum.int8 import INCQuantizer\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"philschmid/llama-2-7b-instruction-generator\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model in float32 precision (since we're using CPU)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32\n",
    ").to('cpu')\n",
    "\n",
    "# Initialize the quantizer\n",
    "quantizer = INCQuantizer.from_pretrained(model_id)\n",
    "\n",
    "# Quantize the model to int8 (This will work on CPU)\n",
    "quantized_model = quantizer.quantize(model, dtype=torch.qint8)\n",
    "\n",
    "# Save the quantized model to disk\n",
    "save_folder = \"quantized_llama\"\n",
    "quantized_model.save_pretrained(save_folder)\n",
    "\n",
    "# Save the tokenizer to the same folder\n",
    "tokenizer.save_pretrained(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the quantized model\n",
    "def generate_text(prompt, max_length=50):\n",
    "    # Load the quantized model\n",
    "    quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "        save_folder, torch_dtype=torch.qint8\n",
    "    ).to(\"cpu\")\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_folder)\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "    # Generate text\n",
    "    outputs = quantized_model.generate(**inputs, max_length=max_length)\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt to test the model\n",
    "prompt = \"The future of AI is\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
